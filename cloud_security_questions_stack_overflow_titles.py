# -*- coding: utf-8 -*-
"""Cloud_security_questions_stack_overflow_titles.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1B7SMEroYSN7TveKGlVLKRRVp1KgtQwBe

This notebook is meant to create a python script that will be generic for all the datasets

**Topic Modeling in Python: Latent Dirichlet Allocation (LDA)**
"""

from google.colab import drive
drive.mount('/content/drive')

## 
import pandas as pd
import os
import re
os.chdir('..')
# Read data into papers
posts_df = pd.read_csv('/content/drive/MyDrive/IM Datasets/BQ/bq-results-20211207-134101-title.csv')
# Print head
posts_df.head(5)

from google.colab import drive
drive.mount('/content/drive')

"""**Data Cleaning**"""

# Remove the un-necessary columns
posts_df = posts_df.drop(columns=['id'], axis=1)
# Print the first couple of rows to verify
posts_df.head()

"""**Remove punctuation/lower casing**"""

## Cleaning Post Body
# Remove punctuation
posts_df['title'] = posts_df['title'].map(lambda x: re.sub('[,\.!?]', '', x))
posts_df['title'] = posts_df['title'].str.replace('<p>' , '')
# Convert the titles to lowercase
posts_df['title'] = posts_df['title'].map(lambda x: x.lower())
# Print out the first rows of Post
posts_df['title'].head()

"""**Exploratory Analysis**"""

# Import the wordcloud library
from wordcloud import WordCloud

# Join the different processed titles together.
long_string_title = ','.join(list(posts_df['title']))

# Create a WordCloud object
wordcloud = WordCloud(background_color="white", max_words=5000, contour_width=3, contour_color='steelblue')

# Generate a word cloud
wordcloud.generate(long_string_title)

# Visualize the word cloud
wordcloud.to_image()

"""**Stop-words function**"""

import gensim
from gensim.utils import simple_preprocess
import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords

stop_words = stopwords.words('english')
stop_words.extend(['flaws'])

def sent_to_words(sentences):
    for sentence in sentences:
        # deacc=True removes punctuations
        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))

def remove_stopwords(texts):
    return [[word for word in simple_preprocess(str(doc)) 
             if word not in stop_words] for doc in texts]

posts = posts_df.title.values.tolist()
post_words = list(sent_to_words(posts))

# remove stop words
post_words = remove_stopwords(post_words)

print(post_words)

print(post_words[:1][0])

"""**Creating dictionary**"""

import gensim.corpora as corpora
# Create Dictionary
id2word = corpora.Dictionary(post_words)
# Create Corpus
texts = post_words
# Term Document Frequency
corpus = [id2word.doc2bow(text) for text in texts]
# View
print(corpus[:1][0][:30])

"""**LDA model training**"""

from pprint import pprint
# number of topics
num_topics = 10
# Build LDA model
lda_model = gensim.models.LdaMulticore(corpus=corpus,
                                       id2word=id2word,
                                       num_topics=num_topics,
                                       random_state=100,
                                       chunksize=100,
                                       passes=10,
                                       per_word_topics=True)
# Print the Keyword in the 10 topics
pprint(lda_model.print_topics())
doc_lda = lda_model[corpus]

"""**Analyzing LDA model results**"""

import pandas as pd
pd.__version__

import pyLDAvis.gensim_models as gensimvis
import pickle 
import pyLDAvis
# Visualize the topics
pyLDAvis.enable_notebook()

LDAvis_prepared = pyLDAvis.gensim_models.prepare(lda_model, corpus, id2word)
LDAvis_prepared